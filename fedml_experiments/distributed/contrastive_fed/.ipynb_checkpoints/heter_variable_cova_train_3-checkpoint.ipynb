{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88457bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import argparse\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"./../../../../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"./../../../\")))\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa4595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9da6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from fedml_api.data_preprocessing.cifar10.data_loader import load_partition_data_cifar10\n",
    "from fedml_api.standalone.fedavg.my_model_trainer_classification import MyModelTrainer as MyModelTrainerCLS\n",
    "from fedml_api.model.contrastive_cv.resnet_with_embedding import Resnet56\n",
    "from CovaMNet2 import CovaMResnet56\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "# from triplet_loss import TripletLoss\n",
    "from hard_triplet_loss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c38d330b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mslimfun\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Cova_hetero_clients10_adam-leaky_relu_iter_with_covaTrue</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/slimfun/cova\" target=\"_blank\">https://wandb.ai/slimfun/cova</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/slimfun/cova/runs/2olvwd4u\" target=\"_blank\">https://wandb.ai/slimfun/cova/runs/2olvwd4u</a><br/>\n",
       "                Run data is saved locally in <code>/home/fw1/nfs_mnt/FedML/fedml_experiments/distributed/contrastive_fed/wandb/run-20211112_015516-2olvwd4u</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "data_dir = \"./../../../data/cifar10\"\n",
    "partition_method = 'hetero'\n",
    "# partition_method = 'homo'\n",
    "partition_alpha = 0.5\n",
    "client_num_in_total = 10\n",
    "batch_size = 100\n",
    "total_epochs = 500\n",
    "\n",
    "save_model_path = 'model/cs_{0}_{1}_client_{2}_variable_covaM_epochs_{3}.pt'\n",
    "\n",
    "device = 'cuda:3'\n",
    "with_cova = True\n",
    "\n",
    "wandb.init(\n",
    "            # project=\"federated_nas\",\n",
    "            project=\"cova\",\n",
    "            name=\"Cova_\" + str(partition_method) + \"_clients\" + str(client_num_in_total) + \"_adam-leaky_relu_iter_with_cova\" + str(\n",
    "                with_cova)\n",
    "        )\n",
    "# config = wandb.config\n",
    "# config.learning_rate = 0.01\n",
    "\n",
    "with open(f'dataset_{partition_method}_{client_num_in_total}.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "# [train_data_num, test_data_num, train_data_global, test_data_global, \\\n",
    "# #             train_data_local_num_dict, train_data_local_dict, test_data_local_dict, \\\n",
    "# #             class_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67995584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# ls = np.array([int(l) for l in labels])\n",
    "# counter_result = Counter(ls)\n",
    "# print(Counter(ls))\n",
    "# print(dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8c4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Resnet56(class_num=dataset[-1], neck='bnneck')\n",
    "# model.load_state_dict(torch.load('model/cs_3_homo_client_0_oral_epochs_200.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6474ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data_dict.pickle', 'rb') as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a16a03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "cl = []\n",
    "for k, v in data_dict.items():\n",
    "#     print(len(v))\n",
    "    cl.append(torch.stack(v).to(device))\n",
    "# print(cl[0].shape)\n",
    "print(len(cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55b468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cal_covariance(input):\n",
    "\n",
    "# #     CovaMatrix_list = []\n",
    "# #     mean_list = []\n",
    "#     B, C, h, w = input.size()\n",
    "# #     print(B)\n",
    "#     support_set_sam = input.permute(1, 0, 2, 3)\n",
    "#     support_set_sam = support_set_sam.contiguous().view(C, -1)\n",
    "    \n",
    "#     mean_support = torch.mean(support_set_sam, 1, True)\n",
    "# #     mean_list.append(mean_support)\n",
    "    \n",
    "#     support_set_sam = support_set_sam-mean_support\n",
    "    \n",
    "#     covariance_matrix = support_set_sam@torch.transpose(support_set_sam, 0, 1)\n",
    "#     covariance_matrix = torch.div(covariance_matrix, h*w*B-1)\n",
    "    \n",
    "#     return covariance_matrix.clone().detach(), mean_support.clone().detach()\n",
    "\n",
    "\n",
    "# def get_cova(model, device):\n",
    "#     cova_list = [0 for i in range(10)]\n",
    "#     mean_list = [0 for i in range(10)]\n",
    "    \n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     features = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for k, v in data_dict.items():\n",
    "#             x = torch.stack(data_dict[k]).to(device)\n",
    "# #             print(x.shape)\n",
    "#             score, feats = model(x)\n",
    "#             covariance_matrix, mean_support = cal_covariance(feats.to(device))\n",
    "#             cova_list[k] = covariance_matrix.to(device)\n",
    "#             mean_list[k] = mean_support.to(device)\n",
    "            \n",
    "#     return cova_list, mean_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5e0fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(object):\n",
    "    def __init__(self, client_index, train_data_local_dict, train_data_local_num_dict, test_data_local_dict, device, model):\n",
    "        self.id = client_index\n",
    "        self.train_data = train_data_local_dict[self.id]\n",
    "        self.local_sample_number = train_data_local_num_dict[self.id]\n",
    "        self.test_local = test_data_local_dict[self.id]\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = model\n",
    "model1 = CovaMResnet56(class_num=dataset[-1], neck='bnneck', with_cova=with_cova)\n",
    "client_1 = Client(0, dataset[5], dataset[4], dataset[6], device, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edaa339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, device, cl, ml):\n",
    "#     cl = cms[0]\n",
    "#     ml = cms[1]\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    metrics = {\n",
    "        'test_correct': 0,\n",
    "        'test_loss': 0,\n",
    "        'test_total': 0,\n",
    "        'correct_cova': 0\n",
    "    }\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(test_data):\n",
    "            x = x.to(device)\n",
    "            target = target.to(device)\n",
    "#             support_ml = []\n",
    "#             for l in target:\n",
    "#                 support_ml.append(ml[l.data].to(device))\n",
    "            pred, pred_ce_cova, feat = model(x, cl, ml=None)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "#             loss_ce_cova = criterion(pred_ce_cova, target)\n",
    "\n",
    "            _, predicted = torch.max(pred, -1)\n",
    "            _, predicted_ce_cova = torch.max(pred_ce_cova, -1)\n",
    "            correct = predicted.eq(target).sum()\n",
    "            correct_ce_cova = predicted_ce_cova.eq(target).sum()\n",
    "\n",
    "            metrics['test_correct'] += correct.item()\n",
    "#             metrics['test_correct'] += correct_ce_cova.item()\n",
    "            metrics['test_loss'] += loss.item() * target.size(0)\n",
    "            metrics['test_total'] += target.size(0)\n",
    "            metrics['correct_cova'] += correct_ce_cova.item()\n",
    "            \n",
    "    return metrics\n",
    "\n",
    "def train_model(client, epochs):\n",
    "#     learning_rate = 0.001\n",
    "#     wd = 0.0001\n",
    "#     learning_rate = 0.00035\n",
    "#     wd = 0.0005\n",
    "\n",
    "#     cl = cms[0]\n",
    "#     ml = cms[1]\n",
    "    \n",
    "    \n",
    "    margin = 0.3\n",
    "    \n",
    "    client.model.to(client.device)\n",
    "    client.model.train()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion2 = nn.CrossEntropyLoss().to(device)\n",
    "    mse_loss = nn.MSELoss().to(device)\n",
    "    \n",
    "#     curr_lr = learning_rate\n",
    "#     optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, client.model.parameters()), lr=curr_lr,\n",
    "#                                          weight_decay=wd, amsgrad=True)\n",
    "#     optimizer = torch.optim.SGD(client.model.parameters(), lr=0.001,\n",
    "#                       momentum=0.9, weight_decay=5e-4)\n",
    "    optimizer = torch.optim.Adam(client.model.parameters(), lr=0.001, betas=(0.5, 0.9))\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "    epoch_loss = []\n",
    "    total_step = len(client.train_data)\n",
    "#     wandb.watch(client.model, log='all')\n",
    "    lbd = 0.1\n",
    "    cls_it = 10000\n",
    "    cova_it = 5000\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "#         if epoch > 0:\n",
    "        \n",
    "#         print(ml[0])\n",
    "        \n",
    "        client.model.train()\n",
    "        batch_loss = []\n",
    "        for batch_idx, (x, labels) in enumerate(client.train_data):\n",
    "            \n",
    "#             cl, ml = get_cova(client.model, client.device)\n",
    "#             if epoch > 0:\n",
    "#                 print(cl[0])\n",
    "            x, labels = x.to(device), labels.to(device)\n",
    "#             print(labels.shape)\n",
    "            client.model.zero_grad()\n",
    "#             support_ml = []\n",
    "#             for l in labels:\n",
    "#                 # B, 256, 64\n",
    "#                 support_ml.append(ml[l.data].to(device))\n",
    "#             print(cl[0].shape)\n",
    "            cls_score, cova_score, feat = client.model(x, support_imgs=cl, ml=None)\n",
    "#             print(score.shape)\n",
    "#             print(cova_score)\n",
    "#             print(cova_score)\n",
    "#             loss = criterion(cova_score, labels).to(device)\n",
    "#             if epoch < 100:\n",
    "            if cls_it > 0:\n",
    "                loss = criterion(cls_score, labels)\n",
    "                cls_it -= 1\n",
    "                lbd += (1-0.1) / (epochs-100)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "            elif cova_it > 0:\n",
    "                loss = criterion2(cova_score, labels).to(device)\n",
    "                cova_it -= 1\n",
    "                \n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                batch_loss.append(loss.item())\n",
    "            else:\n",
    "                cls_it = 10\n",
    "                cova_it = 5\n",
    "#             if epoch > 100 :\n",
    "#             loss = criterion(cova_score, labels).to(device)\n",
    "#             loss = 0.5 * loss.to(device) + criterion2(cova_score, labels).to(device)\n",
    "            \n",
    "        \n",
    "#         print(cl[0])\n",
    "            \n",
    "#         if epoch % 50 == 0:\n",
    "#             torch.save(client.model.state_dict(), str.format(save_model_path, client_num_in_total, partition_method, client.id, epoch))\n",
    "        epoch_loss.append(sum(batch_loss) / len(batch_loss))\n",
    "#         scheduler.step()\n",
    "        print('Client Index = {}\\tEpoch: {}\\tLoss: {:.6f}'.format(\n",
    "            client.id, epoch, sum(batch_loss) / len(batch_loss)))\n",
    "        \n",
    "        metrics = test(client.model, client.test_local, client.device, cl, None)\n",
    "        test_correct = metrics['test_correct']/metrics['test_total']\n",
    "        test_loss = metrics['test_loss']/metrics['test_total']\n",
    "        correct_ce_cova = metrics['correct_cova']/metrics['test_total']\n",
    "#         print(metrics['test_total'])\n",
    "        print(f'test_correct: {test_correct}; test_loss: {test_loss}')\n",
    "        wandb.log({\n",
    "#             'epoch' : epoch,\n",
    "            'train_loss': sum(batch_loss) / len(batch_loss),\n",
    "            'test_correct' : test_correct,\n",
    "            'test_loss' : test_loss,\n",
    "            'correct_cova' : correct_ce_cova\n",
    "        }, step=epoch)\n",
    "        \n",
    "    torch.save(client.model.state_dict(), str.format(save_model_path, client_num_in_total, partition_method, client.id, epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d69b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client Index = 0\tEpoch: 0\tLoss: 1.796494\n",
      "test_correct: 0.2195; test_loss: 2.5130488014221193\n",
      "Client Index = 0\tEpoch: 1\tLoss: 1.540858\n",
      "test_correct: 0.2685; test_loss: 2.2757951176166533\n",
      "Client Index = 0\tEpoch: 2\tLoss: 1.443548\n",
      "test_correct: 0.2658; test_loss: 2.4109543907642363\n",
      "Client Index = 0\tEpoch: 3\tLoss: 1.363813\n",
      "test_correct: 0.2466; test_loss: 2.5617873167991636\n",
      "Client Index = 0\tEpoch: 4\tLoss: 1.335915\n",
      "test_correct: 0.2474; test_loss: 3.0603782534599304\n",
      "Client Index = 0\tEpoch: 5\tLoss: 1.283699\n",
      "test_correct: 0.3194; test_loss: 2.2960650765895845\n",
      "Client Index = 0\tEpoch: 6\tLoss: 1.274201\n",
      "test_correct: 0.2915; test_loss: 2.506047959327698\n",
      "Client Index = 0\tEpoch: 7\tLoss: 1.228957\n",
      "test_correct: 0.3053; test_loss: 3.1096451902389526\n",
      "Client Index = 0\tEpoch: 8\tLoss: 1.175634\n",
      "test_correct: 0.3072; test_loss: 2.484747425317764\n",
      "Client Index = 0\tEpoch: 9\tLoss: 1.143603\n",
      "test_correct: 0.3082; test_loss: 3.0790195178985598\n",
      "Client Index = 0\tEpoch: 10\tLoss: 1.131661\n",
      "test_correct: 0.3443; test_loss: 2.4323293602466585\n",
      "Client Index = 0\tEpoch: 11\tLoss: 1.100970\n",
      "test_correct: 0.3742; test_loss: 2.1444964623451233\n",
      "Client Index = 0\tEpoch: 12\tLoss: 1.061261\n",
      "test_correct: 0.348; test_loss: 2.236799875497818\n",
      "Client Index = 0\tEpoch: 13\tLoss: 1.029666\n",
      "test_correct: 0.3657; test_loss: 2.544232004880905\n",
      "Client Index = 0\tEpoch: 14\tLoss: 1.007542\n",
      "test_correct: 0.3659; test_loss: 2.221383752822876\n",
      "Client Index = 0\tEpoch: 15\tLoss: 1.002459\n",
      "test_correct: 0.3979; test_loss: 2.4025716412067415\n",
      "Client Index = 0\tEpoch: 16\tLoss: 0.969206\n",
      "test_correct: 0.3914; test_loss: 2.366066702604294\n",
      "Client Index = 0\tEpoch: 17\tLoss: 0.935072\n",
      "test_correct: 0.3311; test_loss: 2.9773388576507567\n",
      "Client Index = 0\tEpoch: 18\tLoss: 0.925237\n",
      "test_correct: 0.3975; test_loss: 2.375851045846939\n",
      "Client Index = 0\tEpoch: 19\tLoss: 96.321452\n",
      "test_correct: 0.2469; test_loss: 2.7645365452766417\n",
      "Client Index = 0\tEpoch: 20\tLoss: 20.240457\n",
      "test_correct: 0.1102; test_loss: 9.817203760147095\n",
      "Client Index = 0\tEpoch: 21\tLoss: 3.753384\n",
      "test_correct: 0.0999; test_loss: 15.610209927558898\n",
      "Client Index = 0\tEpoch: 22\tLoss: 2.353712\n",
      "test_correct: 0.1264; test_loss: 31.609660449028016\n",
      "Client Index = 0\tEpoch: 23\tLoss: 2.260050\n",
      "test_correct: 0.1333; test_loss: 77.39875513076782\n",
      "Client Index = 0\tEpoch: 24\tLoss: 1.961968\n",
      "test_correct: 0.1104; test_loss: 93.30112397193909\n",
      "Client Index = 0\tEpoch: 25\tLoss: 1.714526\n"
     ]
    }
   ],
   "source": [
    "train_model(client_1, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
